1. What is data driven testing: 
	Data-driven testing is about testing software with different kinds of data. Instead of putting the data directly into the testing instructions, we keep it separate. 
	This makes it easier to try out different data without having to change the instructions each time. It's like trying on different clothes to see which fits best. 
	This way, testers can run many tests quickly and see if the software works well with different kinds of data.
	For example, an e-learning have student information form where, students will fill the form to get more information about a 
	specific degree in specific area of interest. So to test such forms, we can create a test data with different combinations of degree and
	area of interests which will act as different scenarios as well.


2. Explain steps of bug cycle?
	1. Discovery/Identification: Defects are identified during various phases of software development, including requirements analysis, design, coding, testing, or even during the usage of the software
	2. Logging: Once discovered, defects are logged into a defect tracking system such as Jira. This involves documenting details such as defect description, steps to reproduce, severity, and any relevant attachments.
	3. Triage: Logged defects are reviewed and prioritized based on factors like severity, impact, and urgency. This step helps in allocating resources effectively for fixing defects.
	4. Assignment: Defects are assigned to developers or teams responsible for fixing them. Assignment is based on factors like developer expertise, workload, and the nature of the defect.
	5. Analysis: The assigned developer analyzes the defect to understand its root cause and impact on the software. This may involve replicating the issue, debugging, and reviewing code.
	6. Fixing: Once analyzed, the developer fixes the defect by making necessary changes to the code, configuration, or documentation.
	7. Verification: After fixing, the defect undergoes verification to ensure that the issue has been resolved successfully. This involves retesting the affected functionality to confirm that the defect has been fixed and does not reoccur.
	8. Closure: If verification confirms successful resolution, the defect is marked as closed in the defect tracking system. Relevant stakeholders are notified, and documentation is updated accordingly.
	9. Reopening (if necessary): In some cases, a closed defect may reappear, or the fix may not completely resolve the issue. If this happens, the defect is reopened, and the cycle repeats from the analysis stage.


3. Mention the different types of testing?
	1. Unit Testing: Testing individual units or components of the software in isolation to ensure they work correctly.
	2. Integration Testing: Testing the interactions between integrated units or components to ensure they work together as expected.
	3. System Testing: Testing the entire software system as a whole to validate that it meets specified requirements and functions properly.
	4. Acceptance Testing: Testing conducted to determine whether the software meets user requirements and whether it is acceptable for delivery.
	5. Regression Testing: Testing conducted to ensure that changes or enhancements to the software have not adversely affected existing functionality.
	6. Functional Testing: Testing the functionality of the software by providing input and verifying that the output meets expected results.
	7. Non-Functional Testing: Testing aspects of the software other than its functionality, such as performance, usability, security, reliability, and compatibility.
	8. Performance Testing: Testing the performance characteristics of the software, such as responsiveness, scalability, and stability, under various conditions.
	9. Load Testing: Testing the software's ability to handle expected loads, such as concurrent users or transactions, without performance degradation.
	10. Stress Testing: Testing the software's ability to handle extreme conditions, such as high loads or resource exhaustion, beyond normal operational limits.
	11. Security Testing: Testing the software for vulnerabilities, risks, and potential security breaches to ensure that sensitive data is protected.
	12. Usability Testing: Testing the software's user interface and overall user experience to ensure it is intuitive, easy to use, and meets user expectations.
	13. Compatibility Testing: Testing the software to ensure it functions correctly across different devices, operating systems, browsers, and environments.
	14. Localization Testing: Testing the software to ensure it is adapted to the language, culture, and regional preferences of target users.
	15. Accessibility Testing: Testing the software to ensure it is accessible to users with disabilities and complies with accessibility standards and guidelines.
	

4. What is traceability matrix and what information does it contains?
	A traceability matrix is a structured document that serves as a roadmap connecting various elements of a software project:
	1. **Requirements**: The functionalities the software should have.
	2. **Test Cases**: Scenarios designed to validate these requirements.
	3. **Defects**: Any issues identified during testing.
	It aids in:
	- Identifying which requirements are tested and which are not.
	- Tracking the relationship between requirements and test cases.
	- Understanding which defects are tied to specific requirements.
	By using this matrix, teams ensure comprehensive test coverage, verify that all requirements are fulfilled, 
	and prioritize defect resolution based on their impact on functionality. It serves as a reference point for stakeholders, 
	providing transparency and accountability throughout the development lifecycle.
	Ultimately, the traceability matrix facilitates effective decision-making, mitigates risks,
	and enhances the overall quality and reliability of the software product.


What are automation challenges that testing team faces while testing?
	Tool Selection: Choosing the right automation tools to fit project requirements and team skills.
	Script Maintenance: Keeping automation scripts updated to reflect changes in the application.
	Test Data Management: Generating and managing realistic test data for various scenarios.
	Environment Configuration: Setting up and maintaining consistent test environments.
	Test Case Prioritization: Identifying and prioritizing which test cases to automate first.
	Integration with CI/CD: Ensuring seamless integration of automated tests into CI/CD pipelines.
	Resource Constraints: Managing limited resources, including budget and skilled personnel.
	Test Flakiness: Addressing inconsistencies in automated test results due to various factors.
